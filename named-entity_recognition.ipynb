{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Embedding,Dropout\n",
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "\n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else: \n",
    "            token,tag = line.split()\n",
    "            if(token.startswith('http')):\n",
    "                token = token.replace(token,'<URL>')\n",
    "            if(token.startswith('@')):\n",
    "                token = token.replace(token,'<USR>')\n",
    "\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "    return tokens,tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token,train_tag = read_data('data/train.txt')\n",
    "test_token,test_tag = read_data('data/test.txt')\n",
    "val_token,val_tag = read_data('data/validation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for token,tag in zip(train_token[i],train_tag[i]):\n",
    "        print('%s\\t%s'%(token,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token = '<UNK>')\n",
    "tokenizer.fit_on_texts(train_token+val_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_seq(list_file):\n",
    "    ret_val = []\n",
    "    for s in list_file:\n",
    "        helper = []\n",
    "        for w in s:\n",
    "            helper.append(w)\n",
    "        ret_val.append(' '.join(helper))\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = token_to_seq(train_token)\n",
    "val_seq = token_to_seq(val_token)\n",
    "test_seq = token_to_seq(test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = np.max([len(w) for w in train_seq+val_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train_seq)\n",
    "val_seq = tokenizer.texts_to_sequences(val_seq)\n",
    "# train_seq = tokenizer.texts_to_sequences(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = pad_sequences(train_seq,maxlen = maxlen,padding='post')\n",
    "val_seq = pad_sequences(val_seq,maxlen = maxlen,padding='post')\n",
    "# test_seq = pad_sequences(test_seq,maxlen = maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(word_index) + 1\n",
    "####### input sequence done ##### now go for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_seq(tag_file):\n",
    "    tag = []\n",
    "    for s in tag_file:\n",
    "        helper = []\n",
    "        for w in s:\n",
    "            if(w.startswith('B') or w.startswith('I')):\n",
    "                w = 1\n",
    "            else:\n",
    "                w = 0\n",
    "            helper.append(w)\n",
    "        tag.append(helper)\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_seq = tag_to_seq(train_tag)\n",
    "val_tag_seq = tag_to_seq(val_tag)\n",
    "# train_tag_seq = tag_to_seq(train_tag)val_seq\n",
    "# max_tag_len = np.max([len(w) for w in train_tag_seq+val_tag_seq])\n",
    "train_tag_seq = pad_sequences(train_tag_seq,maxlen = maxlen,padding = 'post')\n",
    "val_tag_seq = pad_sequences(val_tag_seq,maxlen = maxlen,padding = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words,200,input_length = maxlen))\n",
    "    model.add(Bidirectional(LSTM(200)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(maxlen,activation='softmax'))    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 182, 200)          3386600   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 400)               641600    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 182)               72982     \n",
      "=================================================================\n",
      "Total params: 4,101,182\n",
      "Trainable params: 4,101,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5795 samples, validate on 724 samples\n",
      "Epoch 1/4\n",
      "5795/5795 [==============================] - 281s 48ms/sample - loss: 0.0294 - acc: 0.9928 - val_loss: 0.0261 - val_acc: 0.9932\n",
      "Epoch 2/4\n",
      "5795/5795 [==============================] - 284s 49ms/sample - loss: 0.0270 - acc: 0.9928 - val_loss: 0.0247 - val_acc: 0.9932\n",
      "Epoch 3/4\n",
      "5795/5795 [==============================] - 258s 45ms/sample - loss: 0.0251 - acc: 0.9928 - val_loss: 0.0237 - val_acc: 0.9932\n",
      "Epoch 4/4\n",
      "5795/5795 [==============================] - 234s 40ms/sample - loss: 0.0233 - acc: 0.9928 - val_loss: 0.0234 - val_acc: 0.9933\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_seq,train_tag_seq,validation_data=(val_seq,val_tag_seq),batch_size=32,epochs=4,callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_seq)\n",
    "test_seq = pad_sequences(test_seq,maxlen = maxlen,padding = 'post')\n",
    "test_tag_seq = tag_to_seq(test_tag)\n",
    "test_tag_seq = pad_sequences(test_tag_seq,maxlen = maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724/724 [==============================] - 5s 6ms/sample - loss: 0.0263 - acc: 0.9923\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(test_seq,test_tag_seq,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seq = np.array(['hey satya how are you, btw sishrut told me to say hi to you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seq = np.array(tokenizer.texts_to_sequences(my_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 370,    1,  117,   59,   16, 2448,    1,  585,   40,    8,  178,\n",
       "         534,    8,   16]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seq = pad_sequences(my_seq,maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Seq_val = model.predict(my_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
